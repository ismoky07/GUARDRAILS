from typing import Iterator, Optional
import os
import time
from dotenv import load_dotenv
from agno.agent import Agent, RunResponse
from agno.models.xai import xAI
from agno.tools.yfinance import YFinanceTools
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from rich.pretty import pprint

load_dotenv()
XAI_API_KEY = os.getenv("XAI_API_KEY")

# Cr√©er l'agent pour l'√©valuation
agent = Agent(
    model=xAI(id="grok-3", api_key=XAI_API_KEY),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
    instructions="""Tu es un agent financier.
    Ton r√¥le est de choisir les bons outils pour chaque t√¢che.
    Utilise get_current_stock_price pour les requ√™tes boursi√®res.
    Analyse chaque demande pour s√©lectionner l'outil appropri√©."""
)

# D√©finir les t√¢ches d'√©valuation pour la Tool Selection Accuracy
evaluation_tasks = [
    {
        "name": "Correct Tool Selection - Stock Price",
        "input": "What is the current stock price of NVDA?",
        "expected_output": "The agent should use the get_current_stock_price tool to fetch NVDA stock price",
        "additional_guidelines": "The agent must correctly identify that this is a stock price query and use the appropriate YFinance tool."
    },
    {
        "name": "Correct Tool Selection - Multiple Stocks",
        "input": "Compare the stock prices of NVDA and AAPL",
        "expected_output": "The agent should use the get_current_stock_price tool twice - once for NVDA and once for AAPL",
        "additional_guidelines": "The agent must recognize this requires multiple tool calls to get both stock prices for comparison."
    },
    {
        "name": "Tool Selection for Invalid Symbol",
        "input": "What is the current stock price of INVALID_SYMBOL?",
        "expected_output": "The agent should attempt to use the get_current_stock_price tool but handle the error gracefully",
        "additional_guidelines": "The agent should still try to use the appropriate tool even for invalid symbols, then handle the error response."
    }
]

print("üîß D√âMARRAGE DES TESTS DE TOOL SELECTION ACCURACY")
print("="*70)

all_results = []
tool_selection_metrics = []

for i, task_data in enumerate(evaluation_tasks, 1):
    print(f"\nüìã √âVALUATION {i}: {task_data['name']}")
    print(f"üìù INPUT: {task_data['input']}")
    print(f"üéØ OUTPUT ATTENDU: {task_data['expected_output']}")
    print("-" * 60)
    
    # Mesurer le temps de l'√©valuation
    evaluation_start_time = time.time()
    
    # Cr√©er l'√©valuation AccuracyEval
    evaluation = AccuracyEval(
        name=task_data['name'],
        model=xAI(id="grok-3", api_key=XAI_API_KEY),  # Mod√®le pour l'√©valuation
        agent=agent,  # Agent √† √©valuer
        input=task_data['input'],
        expected_output=task_data['expected_output'],
        additional_guidelines=task_data['additional_guidelines'],
        num_iterations=1,  # Nombre d'it√©rations pour la robustesse
    )
    
    # Ex√©cuter l'√©valuation
    result: Optional[AccuracyResult] = evaluation.run(print_results=True)
    
    # Mesurer le temps de fin
    evaluation_end_time = time.time()
    total_evaluation_time = evaluation_end_time - evaluation_start_time
    
    # Collecter les m√©triques
    if result is not None:
        # R√©cup√©rer les scores individuels depuis les r√©sultats d√©taill√©s
        individual_scores = []
        if hasattr(result, 'results') and result.results:
            for eval_result in result.results:
                if hasattr(eval_result, 'score'):
                    individual_scores.append(eval_result.score)
        
        task_metrics = {
            'task_id': i,
            'task_name': task_data['name'],
            'input': task_data['input'],
            'expected_output': task_data['expected_output'],
            'avg_score': result.avg_score,
            'scores': individual_scores,
            'iterations': len(individual_scores) if individual_scores else 0,
            'execution_time': total_evaluation_time,
            'success': result.avg_score >= 7.0  # Seuil de succ√®s
        }
        
        tool_selection_metrics.append(task_metrics)
           
        # Afficher les r√©sultats
        print(f"üîß Score de s√©lection d'outils: {result.avg_score:.2f}/10")
        if individual_scores:
            print(f"   ‚Ä¢ Scores individuels: {individual_scores}")
            print(f"   ‚Ä¢ Score max: {max(individual_scores):.2f}")
            print(f"   ‚Ä¢ Score min: {min(individual_scores):.2f}")
        else:
            print(f"   ‚Ä¢ Scores individuels: Non disponibles")
        print(f"   ‚Ä¢ Succ√®s: {'‚úÖ' if result.avg_score >= 7.0 else '‚ùå'}")
        print(f"   ‚Ä¢ Temps d'ex√©cution: {total_evaluation_time:.3f}s")
        
    else:
        print("‚ùå √âchec de l'√©valuation - Aucun r√©sultat obtenu")
        task_metrics = {
            'task_id': i,
            'task_name': task_data['name'],
            'input': task_data['input'],
            'expected_output': task_data['expected_output'],
            'avg_score': 0.0,
            'scores': [],
            'iterations': 0,
            'execution_time': total_evaluation_time,
            'success': False
        }
        tool_selection_metrics.append(task_metrics)

# Analyse globale de la Tool Selection Accuracy
print("\n" + "="*80)
print("üîß ANALYSE DE LA TOOL SELECTION ACCURACY")
print("="*80)

if tool_selection_metrics:
    total_avg_score = sum(m['avg_score'] for m in tool_selection_metrics)
    average_accuracy = total_avg_score / len(tool_selection_metrics)
    total_execution_time = sum(m['execution_time'] for m in tool_selection_metrics)
    successful_tasks = sum(1 for m in tool_selection_metrics if m['success'])
    success_rate = (successful_tasks / len(tool_selection_metrics)) * 100

    print(f"\nüîß R√âSULTATS GLOBAUX:")
    print(f"   ‚Ä¢ Score moyen de s√©lection d'outils: {average_accuracy:.2f}/10")
    print(f"   ‚Ä¢ Score total: {total_avg_score:.2f}")
    print(f"   ‚Ä¢ Taux de succ√®s: {success_rate:.1f}% ({successful_tasks}/{len(tool_selection_metrics)})")
    print(f"   ‚Ä¢ Temps total d'ex√©cution: {total_execution_time:.3f}s")
    print(f"   ‚Ä¢ Nombre d'√©valuations: {len(tool_selection_metrics)}")

    # Analyse par composant
    all_scores = []
    for m in tool_selection_metrics:
        all_scores.extend(m['scores'])
    
    if all_scores:
        max_score = max(all_scores)
        min_score = min(all_scores)
        score_std = (sum((s - average_accuracy)**2 for s in all_scores) / len(all_scores))**0.5
        
        print(f"\nüîç ANALYSE D√âTAILL√âE:")
        print(f"   ‚Ä¢ Score maximum: {max_score:.2f}/10")
        print(f"   ‚Ä¢ Score minimum: {min_score:.2f}/10")
        print(f"   ‚Ä¢ √âcart-type: {score_std:.2f}")
        print(f"   ‚Ä¢ Total d'it√©rations: {len(all_scores)}")

    # Analyse par t√¢che
    print(f"\nüìã ANALYSE PAR T√ÇCHE:")
    for m in tool_selection_metrics:
        status = "‚úÖ" if m['success'] else "‚ùå"
        print(f"   ‚Ä¢ {m['task_name']}: {m['avg_score']:.2f}/10 {status}")

    # Analyse des types de s√©lection d'outils
    print(f"\nüõ†Ô∏è ANALYSE DES TYPES DE S√âLECTION:")
    correct_tool_tasks = [m for m in tool_selection_metrics if "Correct Tool" in m['task_name']]
    no_tool_tasks = [m for m in tool_selection_metrics if "No Tool" in m['task_name']]
    error_handling_tasks = [m for m in tool_selection_metrics if "Invalid" in m['task_name']]
    
    complex_tasks = [m for m in tool_selection_metrics if "Multiple" in m['task_name'] or "Complex" in m['task_name']]
    
    if correct_tool_tasks:
        avg_correct = sum(m['avg_score'] for m in correct_tool_tasks) / len(correct_tool_tasks)
        print(f"   ‚Ä¢ S√©lection correcte d'outils: {avg_correct:.2f}/10")
    
    if no_tool_tasks:
        avg_no_tool = sum(m['avg_score'] for m in no_tool_tasks) / len(no_tool_tasks)
        print(f"   ‚Ä¢ Refus d'outils inappropri√©s: {avg_no_tool:.2f}/10")
    
    if error_handling_tasks:
        avg_error = sum(m['avg_score'] for m in error_handling_tasks) / len(error_handling_tasks)
        print(f"   ‚Ä¢ Gestion d'erreurs d'outils: {avg_error:.2f}/10")
    
    if complex_tasks:
        avg_complex = sum(m['avg_score'] for m in complex_tasks) / len(complex_tasks)
        print(f"   ‚Ä¢ S√©lection d'outils complexes: {avg_complex:.2f}/10")

else:
    print("‚ùå Aucune m√©trique de s√©lection d'outils disponible")

print("="*80)
print("üîß √âvaluation termin√©e - Consultez les r√©sultats ci-dessus")
print("="*80)
