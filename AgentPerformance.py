import os
import re
import sys
import time
import threading
from typing import Dict, List, Any
from dataclasses import dataclass
from textwrap import dedent
from dotenv import load_dotenv
from agno.agent import Agent
from agno.models.xai import xAI

load_dotenv()

@dataclass
class PerformanceMetric:
    """Structure pour stocker les m√©triques de performance"""
    type: str
    value: float
    unit: str
    timestamp: float
    context: Dict[str, Any]

class PerformanceTools:
    """Outils de performance pour les 4 mesures principales"""
    
    def __init__(self):
        # Historique des interventions humaines
        self.human_interventions = []
        self.intervention_count = 0
        
        # Suivi des √©tapes par t√¢che
        self.task_steps = {}
        self.current_task_id = None
        self.step_count = 0
        
        # Suivi des co√ªts
        self.cost_tracking = {
            'api_calls': 0,
            'tokens_used': 0,
            'storage_used': 0,
            'compute_time': 0
        }
        
        # D√©tection de boucles infinies
        self.loop_detection = {
            'call_history': [],
            'max_calls': 100,
            'timeout_threshold': 30,  # secondes
            'pattern_detection': True
        }
        
        # Co√ªts par type d'op√©ration (exemples)
        self.cost_rates = {
            'api_call': 0.001,  # $0.001 par appel API
            'token': 0.0001,    # $0.0001 par token
            'storage_mb': 0.01, # $0.01 par MB de stockage
            'compute_second': 0.05  # $0.05 par seconde de calcul
        }
    
    def track_human_intervention(self, reason: str, task_id: str = None) -> None:
        """Enregistre une intervention humaine"""
        intervention = {
            'id': len(self.human_interventions) + 1,
            'reason': reason,
            'task_id': task_id or self.current_task_id,
            'timestamp': time.time(),
            'context': {
                'step_count': self.step_count,
                'cost_so_far': self.get_total_cost()
            }
        }
        
        self.human_interventions.append(intervention)
        self.intervention_count += 1
        
        print(f"üîß Mesure Intervention humaine #{self.intervention_count}: {reason}")
    
    def start_task(self, task_id: str, description: str = None) -> None:
        """D√©marre le suivi d'une nouvelle t√¢che"""
        self.current_task_id = task_id
        self.step_count = 0
        
        self.task_steps[task_id] = {
            'description': description,
            'start_time': time.time(),
            'steps': [],
            'interventions': 0,
            'cost': 0
        }
        
        print(f"üìã Mesure Nouvelle t√¢che d√©marr√©e: {task_id}")
    
    def add_step(self, step_description: str, step_type: str = "general") -> None:
        """Ajoute une √©tape √† la t√¢che actuelle"""
        if not self.current_task_id:
            return
        
        step = {
            'step_number': self.step_count + 1,
            'description': step_description,
            'type': step_type,
            'timestamp': time.time(),
            'cost': 0.0  # Co√ªt par d√©faut pour l'√©tape
        }
        
        self.task_steps[self.current_task_id]['steps'].append(step)
        self.step_count += 1
        
        print(f"üìù Mesure √âtape {self.step_count}: {step_description}")
    
    def end_task(self, success: bool = True) -> Dict[str, Any]:
        """Termine la t√¢che actuelle et retourne les m√©triques"""
        if not self.current_task_id:
            return {}
        
        task = self.task_steps[self.current_task_id]
        task['end_time'] = time.time()
        task['duration'] = task['end_time'] - task['start_time']
        task['success'] = success
        task['total_steps'] = len(task['steps'])
        task['total_cost'] = self.get_total_cost()
        
        # Calculer les m√©triques
        metrics = {
            'task_id': self.current_task_id,
            'total_steps': task['total_steps'],
            'duration': task['duration'],
            'success': success,
            'cost': task['total_cost'],
            'interventions': task['interventions'],
            'steps_per_minute': task['total_steps'] / (task['duration'] / 60) if task['duration'] > 0 else 0
        }
        
        print(f"‚úÖ Mesure T√¢che termin√©e: {self.current_task_id}")
        print(f"üìä Mesure √âtapes totales: {task['total_steps']}")
        print(f"‚è±Ô∏è Mesure Dur√©e: {task['duration']:.2f}s")
        print(f"üí∞ Mesure Co√ªt: ${task['total_cost']:.4f}")
        
        self.current_task_id = None
        self.step_count = 0
        
        return metrics
    
    def track_api_call(self, tokens_used: int, call_type: str = "api") -> None:
        """Enregistre un appel API et ses co√ªts"""
        self.cost_tracking['api_calls'] += 1
        self.cost_tracking['tokens_used'] += tokens_used
        
        cost = self.cost_rates['api_call'] + (tokens_used * self.cost_rates['token'])
        self.cost_tracking['compute_time'] += 0.1  # Estimation du temps d'API
        
        print(f"üîå Mesure Appel API: {tokens_used} tokens, co√ªt: ${cost:.4f}")
    
    def track_storage(self, size_mb: float) -> None:
        """Enregistre l'utilisation du stockage"""
        self.cost_tracking['storage_used'] += size_mb
        cost = size_mb * self.cost_rates['storage_mb']
        
        print(f"üíæ Mesure Stockage: {size_mb}MB, co√ªt: ${cost:.4f}")
    
    def track_compute_time(self, duration_seconds: float) -> None:
        """Enregistre le temps de calcul"""
        self.cost_tracking['compute_time'] += duration_seconds
        cost = duration_seconds * self.cost_rates['compute_second']
        
        print(f"‚ö° Mesure Calcul: {duration_seconds:.2f}s, co√ªt: ${cost:.4f}")
    
    def get_total_cost(self) -> float:
        """Calcule le co√ªt total"""
        api_cost = self.cost_tracking['api_calls'] * self.cost_rates['api_call']
        token_cost = self.cost_tracking['tokens_used'] * self.cost_rates['token']
        storage_cost = self.cost_tracking['storage_used'] * self.cost_rates['storage_mb']
        compute_cost = self.cost_tracking['compute_time'] * self.cost_rates['compute_second']
        
        return api_cost + token_cost + storage_cost + compute_cost
    
    
    def detect_infinite_loop(self, function_name: str, args: Dict[str, Any]) -> bool:
        """D√©tecte les boucles infinies"""
        current_time = time.time()
        
        # Ajouter l'appel √† l'historique
        call = {
            'function': function_name,
            'args': args,
            'timestamp': current_time
        }
        self.loop_detection['call_history'].append(call)
        
        # Garder seulement les derniers appels
        if len(self.loop_detection['call_history']) > self.loop_detection['max_calls']:
            self.loop_detection['call_history'] = self.loop_detection['call_history'][-self.loop_detection['max_calls']:]
        
        # D√©tecter les patterns r√©p√©titifs
        if self.loop_detection['pattern_detection']:
            recent_calls = self.loop_detection['call_history'][-10:]  # Derniers 10 appels
            
            if len(recent_calls) >= 5:
                # V√©rifier si la m√™me fonction est appel√©e avec les m√™mes arguments
                function_counts = {}
                for call in recent_calls:
                    key = f"{call['function']}_{hash(str(call['args']))}"
                    function_counts[key] = function_counts.get(key, 0) + 1
                
                # Si une fonction est appel√©e plus de 3 fois avec les m√™mes arguments
                if max(function_counts.values()) > 3:
                    print(f"üîÑ Mesure Boucle infinie d√©tect√©e: {function_name}")
                    return True
        
        # D√©tecter les timeouts
        if len(self.loop_detection['call_history']) > 1:
            time_since_start = current_time - self.loop_detection['call_history'][0]['timestamp']
            if time_since_start > self.loop_detection['timeout_threshold']:
                print(f"‚è∞ Mesure Timeout d√©tect√©: {time_since_start:.2f}s")
                return True
        
        return False
    
    def get_performance_report(self) -> Dict[str, Any]:
        """G√©n√®re un rapport de performance complet"""
        total_tasks = len(self.task_steps)
        completed_tasks = sum(1 for task in self.task_steps.values() if 'end_time' in task)
        
        avg_steps_per_task = 0
        avg_duration_per_task = 0
        avg_cost_per_task = 0
        
        if completed_tasks > 0:
            completed_task_data = [task for task in self.task_steps.values() if 'end_time' in task]
            avg_steps_per_task = sum(task['total_steps'] for task in completed_task_data) / completed_tasks
            avg_duration_per_task = sum(task['duration'] for task in completed_task_data) / completed_tasks
            avg_cost_per_task = sum(task['total_cost'] for task in completed_task_data) / completed_tasks
        
        report = {
            'total_tasks': total_tasks,
            'completed_tasks': completed_tasks,
            'total_interventions': self.intervention_count,
            'total_cost': self.get_total_cost(),
            'avg_steps_per_task': avg_steps_per_task,
            'avg_duration_per_task': avg_duration_per_task,
            'avg_cost_per_task': avg_cost_per_task,
            'intervention_rate': self.intervention_count / completed_tasks if completed_tasks > 0 else 0,
            'cost_breakdown': {
                'api_calls': self.cost_tracking['api_calls'],
                'tokens_used': self.cost_tracking['tokens_used'],
                'storage_mb': self.cost_tracking['storage_used'],
                'compute_seconds': self.cost_tracking['compute_time']
            },
            'recent_interventions': self.human_interventions[-5:],  # 5 derni√®res interventions
            'task_summary': list(self.task_steps.values())[-5:]  # 5 derni√®res t√¢ches
        }
        
        return report


class PerformanceAgent:
    """Agent de performance principal avec toutes les fonctionnalit√©s"""
    
    def __init__(self):
        self.performance_tools = PerformanceTools()
        self.api_key = os.getenv("XAI_API_KEY")
        
        # Cr√©er l'agent avec les outils de performance
        self.agent = Agent(
            name="Performance Assistant",
            model=xAI(id="grok-3-mini", api_key=self.api_key),
            tools=[self.performance_tools],
            markdown=True,
            show_tool_calls=True,
            instructions=dedent("""
            Tu es un agent de performance sp√©cialis√© dans la mesure et l'optimisation.
            Ton r√¥le est de :
            1. Suivre le nombre d'interventions humaines
            2. Compter les √©tapes par t√¢che
            3. Calculer les co√ªts par requ√™te
            4. D√©tecter les boucles infinies
            
            Toujours prioriser l'efficacit√© et la performance.
            """)
        )
    
    def start_performance_tracking(self, task_id: str, description: str = None) -> None:
        """D√©marre le suivi de performance pour une t√¢che"""
        self.performance_tools.start_task(task_id, description)
    
    def add_performance_step(self, step_description: str, step_type: str = "general") -> None:
        """Ajoute une √©tape de performance"""
        self.performance_tools.add_step(step_description, step_type)
    
    def track_human_intervention(self, reason: str) -> None:
        """Enregistre une intervention humaine"""
        self.performance_tools.track_human_intervention(reason)
    
    def end_performance_tracking(self, success: bool = True) -> Dict[str, Any]:
        """Termine le suivi de performance et retourne les m√©triques"""
        return self.performance_tools.end_task(success)
    
    def run_performance_analysis(self, query: str) -> str:
        """Ex√©cute une analyse de performance avec l'agent complet"""
        print("üõ°Ô∏è ANALYSE DE PERFORMANCE AVEC AGENT")
        print("="*50)
        
        # D√©marrer le suivi
        task_id = f"analysis_{int(time.time())}"
        self.start_performance_tracking(task_id, f"Analyse: {query}")
        
        try:
            # Ajouter une √©tape
            self.add_performance_step("D√©marrage de l'analyse", "analysis")
            
            # Utiliser l'agent pour traiter la requ√™te
            response = self.agent.run(query)
            
            # Ajouter une √©tape
            self.add_performance_step("Traitement de la requ√™te", "processing")
            
            # Simuler un appel API
            self.performance_tools.track_api_call(100, "analysis")
            
            print(f"üìã Requ√™te: {query}")
            print(f"üîç R√©ponse: {response.content}")
            
            # Terminer le suivi
            metrics = self.end_performance_tracking(True)
            
            return response.content
        except Exception as e:
            # Enregistrer l'intervention humaine
            self.track_human_intervention(f"Erreur lors de l'analyse: {str(e)}")
            
            error_msg = f"Erreur lors de l'analyse de performance: {str(e)}"
            print(f"‚ùå {error_msg}")
            
            # Terminer le suivi avec √©chec
            self.end_performance_tracking(False)
            
            return error_msg
    
    def search_performance_news(self, topic: str) -> str:
        """Recherche des actualit√©s de performance avec Google Search"""
        print(f"üîç RECHERCHE D'ACTUALIT√âS DE PERFORMANCE: {topic}")
        print("="*50)
        
        # D√©marrer le suivi
        task_id = f"search_{int(time.time())}"
        self.start_performance_tracking(task_id, f"Recherche: {topic}")
        
        try:
            # Ajouter une √©tape
            self.add_performance_step("D√©marrage de la recherche", "search")
            
            # Utiliser l'agent avec Google Search pour les actualit√©s
            query = f"Recherche les derni√®res actualit√©s et tendances de performance concernant {topic} en utilisant Google Search. Inclus les benchmarks et les optimisations."
            response = self.agent.run(query)
            
            # Ajouter une √©tape
            self.add_performance_step("Traitement des r√©sultats", "processing")
            
            # Simuler un appel API
            self.performance_tools.track_api_call(150, "search")
            
            # Terminer le suivi
            self.end_performance_tracking(True)
            
            return response.content
        except Exception as e:
            # Enregistrer l'intervention humaine
            self.track_human_intervention(f"Erreur lors de la recherche: {str(e)}")
            
            error_msg = f"Erreur lors de la recherche d'actualit√©s: {str(e)}"
            print(f"‚ùå {error_msg}")
            
            # Terminer le suivi avec √©chec
            self.end_performance_tracking(False)
            
            return error_msg
    
    def search_performance_metrics(self, metric_type: str) -> str:
        """Recherche des m√©triques de performance sp√©cifiques avec Google Search"""
        print(f"üîç RECHERCHE DE M√âTRIQUES DE PERFORMANCE: {metric_type}")
        print("="*50)
        
        # D√©marrer le suivi
        task_id = f"metrics_{int(time.time())}"
        self.start_performance_tracking(task_id, f"M√©triques: {metric_type}")
        
        try:
            # Ajouter une √©tape
            self.add_performance_step("D√©marrage de la recherche de m√©triques", "search")
            
            query = f"Recherche les m√©triques de performance concernant {metric_type} en utilisant Google Search. Inclus les benchmarks et les outils de mesure."
            response = self.agent.run(query)
            
            # Ajouter une √©tape
            self.add_performance_step("Traitement des m√©triques", "processing")
            
            # Simuler un appel API
            self.performance_tools.track_api_call(120, "metrics")
            
            # Terminer le suivi
            self.end_performance_tracking(True)
            
            return response.content
        except Exception as e:
            # Enregistrer l'intervention humaine
            self.track_human_intervention(f"Erreur lors de la recherche de m√©triques: {str(e)}")
            
            error_msg = f"Erreur lors de la recherche de m√©triques: {str(e)}"
            print(f"‚ùå {error_msg}")
            
            # Terminer le suivi avec √©chec
            self.end_performance_tracking(False)
            
            return error_msg
    
    def get_performance_timestamp(self) -> str:
        """Obtient l'horodatage pour les audits de performance"""
        print("‚è∞ HORODATAGE POUR AUDIT DE PERFORMANCE")
        print("="*50)
        
        try:
            query = "Quelle est la date et l'heure actuelle pour l'audit de performance ?"
            response = self.agent.run(query)
            
            return response.content
        except Exception as e:
            error_msg = f"Erreur lors de l'obtention de l'horodatage: {str(e)}"
            print(f"‚ùå {error_msg}")
            return error_msg
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Obtient le rapport de performance complet"""
        print("üìä RAPPORT DE PERFORMANCE")
        print("="*50)
        
        report = self.performance_tools.get_performance_report()
        
        print(f"üìà Mesure T√¢ches totales: {report['total_tasks']}")
        print(f"‚úÖ Mesure T√¢ches termin√©es: {report['completed_tasks']}")
        print(f"üîß Mesure Interventions humaines: {report['total_interventions']}")
        print(f"üí∞ Mesure Co√ªt total: ${report['total_cost']:.4f}")
        print(f"üìù Mesure √âtapes moyennes par t√¢che: {report['avg_steps_per_task']:.2f}")
        print(f"‚è±Ô∏è Mesure Dur√©e moyenne par t√¢che: {report['avg_duration_per_task']:.2f}s")
        print(f"üíµ Mesure Co√ªt moyen par t√¢che: ${report['avg_cost_per_task']:.4f}")
        print(f"üîß Mesure Taux d'intervention: {report['intervention_rate']:.2%}")
        
        return report
    
    def detect_infinite_loop(self, function_name: str, args: Dict[str, Any]) -> bool:
        """D√©tecte les boucles infinies"""
        return self.performance_tools.detect_infinite_loop(function_name, args)
    
    def secure_code_execution(self, code: str) -> Dict[str, Any]:
        """Ex√©cute du code de mani√®re s√©curis√©e dans un environnement isol√©"""
        print("üîí EX√âCUTION S√âCURIS√âE DU CODE")
        print("="*40)
        
        # D√©marrer le suivi
        task_id = f"execution_{int(time.time())}"
        self.start_performance_tracking(task_id, "Ex√©cution de code")
        
        try:
            # Ajouter une √©tape
            self.add_performance_step("Analyse du code", "analysis")
            
            # D√©tecter les boucles infinies potentielles
            if self.detect_infinite_loop("code_execution", {"code": code}):
                self.track_human_intervention("Boucle infinie d√©tect√©e dans le code")
                return {
                    'success': False,
                    'error': 'Boucle infinie d√©tect√©e',
                    'intervention_required': True
                }
            
            # Ajouter une √©tape
            self.add_performance_step("Ex√©cution dans l'environnement isol√©", "execution")
            
            # Ex√©cuter dans l'environnement isol√©
            result = self.isolated_env.execute_in_isolation(code)
            
            # Ajouter une √©tape
            self.add_performance_step("Traitement des r√©sultats", "processing")
            
            # Simuler le co√ªt de calcul
            self.performance_tools.track_compute_time(1.0)
            
            print(f"‚úÖ Mesure Ex√©cution termin√©e: {result['success']}")
            if result['stdout']:
                print(f"üì§ Mesure Sortie: {result['stdout']}")
            if result['stderr']:
                print(f"‚ùå Mesure Erreurs: {result['stderr']}")
            
            # Terminer le suivi
            self.end_performance_tracking(result['success'])
            
            return result
        except Exception as e:
            # Enregistrer l'intervention humaine
            self.track_human_intervention(f"Erreur lors de l'ex√©cution: {str(e)}")
            
            error_msg = f"Erreur lors de l'ex√©cution: {str(e)}"
            print(f"‚ùå {error_msg}")
            
            # Terminer le suivi avec √©chec
            self.end_performance_tracking(False)
            
            return {
                'success': False,
                'error': error_msg,
                'intervention_required': True
            }
    
    def cleanup(self):
        """Nettoie les ressources"""
        self.isolated_env.cleanup()

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er l'agent de performance
    performance_agent = PerformanceAgent()
    
    print("üõ°Ô∏è AGENT DE PERFORMANCE - 4 MESURES PRINCIPALES")
    print("="*60)
    
    # Test 1: Suivi des interventions humaines
    print("\nüìã MESURE 1: HUMAN INTERVENTIONS TRACKING")
    print("-" * 40)
    performance_agent.start_performance_tracking("task_1", "Test des interventions humaines")
    performance_agent.track_human_intervention("Correction d'erreur de validation")
    performance_agent.track_human_intervention("Ajustement de param√®tres")
    performance_agent.track_human_intervention("Validation manuelle des r√©sultats")
    performance_agent.add_performance_step("√âtape de validation", "validation")
    metrics_1 = performance_agent.end_performance_tracking(True)
    print(f"Mesure Interventions humaines: {metrics_1.get('interventions', 0)}")
    print(f"Mesure Dur√©e de la t√¢che: {metrics_1.get('duration', 0):.2f}s")
    
    # Test 2: Suivi des √©tapes par t√¢che
    print("\nüìã MESURE 2: STEPS PER TASK TRACKING")
    print("-" * 40)
    performance_agent.start_performance_tracking("task_2", "Test des √©tapes par t√¢che")
    performance_agent.add_performance_step("Initialisation", "init")
    performance_agent.add_performance_step("Traitement des donn√©es", "processing")
    performance_agent.add_performance_step("Validation des r√©sultats", "validation")
    performance_agent.add_performance_step("Optimisation", "optimization")
    performance_agent.add_performance_step("Finalisation", "finalization")
    metrics_2 = performance_agent.end_performance_tracking(True)
    print(f"Mesure √âtapes par t√¢che: {metrics_2.get('total_steps', 0)}")
    print(f"Mesure Dur√©e de la t√¢che: {metrics_2.get('duration', 0):.2f}s")
    
    # Test 3: Suivi des co√ªts par requ√™te
    print("\nüìã MESURE 3: COST PER REQUEST TRACKING")
    print("-" * 40)
    performance_agent.start_performance_tracking("task_3", "Test des co√ªts par requ√™te")
    performance_agent.performance_tools.track_api_call(150, "api_call")
    performance_agent.performance_tools.track_api_call(200, "api_call")
    performance_agent.performance_tools.track_storage(2.5)
    performance_agent.performance_tools.track_storage(1.8)
    performance_agent.performance_tools.track_compute_time(1.2)
    performance_agent.performance_tools.track_compute_time(0.8)
    performance_agent.add_performance_step("Calcul des co√ªts", "cost_calculation")
    metrics_3 = performance_agent.end_performance_tracking(True)
    print(f"Mesure Co√ªt total: ${metrics_3.get('cost', 0):.4f}")
    print(f"Mesure Appels API: {performance_agent.performance_tools.cost_tracking['api_calls']}")
    print(f"Mesure Tokens utilis√©s: {performance_agent.performance_tools.cost_tracking['tokens_used']}")
    
    # Test 4: D√©tection de boucles infinies
    print("\nüìã MESURE 4: INFINITE LOOP DETECTION")
    print("-" * 40)
    # Simuler des appels r√©p√©titifs
    loop_detected = False
    for i in range(5):
        loop_detected = performance_agent.performance_tools.detect_infinite_loop("test_function", {"param": "value"})
        if loop_detected:
            print(f"Mesure Boucle infinie d√©tect√©e √† l'it√©ration {i+1}")
            break
    if not loop_detected:
        print("Mesure Aucune boucle infinie d√©tect√©e")
    
    # Test 5: Test combin√© avec toutes les m√©triques
    print("\nüìã MESURE 5: TEST COMBIN√â - TOUTES LES M√âTRIQUES")
    print("-" * 40)
    performance_agent.start_performance_tracking("task_combined", "Test combin√©")
    performance_agent.track_human_intervention("Intervention pour optimisation")
    performance_agent.add_performance_step("√âtape 1", "step1")
    performance_agent.add_performance_step("√âtape 2", "step2")
    performance_agent.performance_tools.track_api_call(100, "combined")
    performance_agent.performance_tools.track_storage(1.0)
    performance_agent.performance_tools.track_compute_time(0.5)
    metrics_combined = performance_agent.end_performance_tracking(True)
    print(f"Mesure Total √©tapes: {metrics_combined.get('total_steps', 0)}")
    print(f"Mesure Interventions: {metrics_combined.get('interventions', 0)}")
    print(f"Mesure Co√ªt total: ${metrics_combined.get('cost', 0):.4f}")
    print(f"Mesure Dur√©e: {metrics_combined.get('duration', 0):.2f}s")
    
    print("\n‚úÖ Tests de performance termin√©s!")
    print("="*60)
