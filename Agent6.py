from typing import Iterator, Optional
import os
import time
from dotenv import load_dotenv
from agno.agent import Agent, RunResponse
from agno.models.xai import xAI
from agno.tools.yfinance import YFinanceTools
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from rich.pretty import pprint

load_dotenv()
XAI_API_KEY = os.getenv("XAI_API_KEY")

# Cr√©er l'agent pour l'√©valuation
agent = Agent(
    model=xAI(id="grok-3", api_key=XAI_API_KEY),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
    instructions="""Tu es un agent financier.
    Ton r√¥le est de fournir des r√©ponses pr√©cises sur les prix d'actions.
    Utilise get_current_stock_price pour toutes les requ√™tes boursi√®res.
    Fournis des r√©ponses d√©taill√©es et professionnelles."""
)

# D√©finir les t√¢ches d'√©valuation avec AccuracyEval
evaluation_tasks = [
    {
        "name": "NVDA Stock Price Accuracy",
        "input": "What is the current stock price of NVDA?",
        "expected_output": "The current stock price of NVDA is $[amount]",
        "additional_guidelines": "The response should include the exact current stock price with dollar sign and should be accurate to the current market data."
    },
    {
        "name": "Stock Comparison Accuracy", 
        "input": "Compare the stock prices of NVDA and AAPL",
        "expected_output": "NVDA: $[price] | AAPL: $[price] | Difference: $[diff]",
        "additional_guidelines": "The response should provide both stock prices and calculate the difference between them."
    },
    {
        "name": "Invalid Symbol Handling",
        "input": "What is the current stock price of INVALID_SYMBOL?",
        "expected_output": "I'm sorry, but INVALID_SYMBOL is not a valid stock symbol",
        "additional_guidelines": "The agent should gracefully handle invalid stock symbols and provide a helpful error message."
    },

]

print("üéØ D√âMARRAGE DES TESTS D'ACCURACY AVEC AGNO FRAMEWORK")
print("="*70)

all_results = []
accuracy_metrics = []

for i, task_data in enumerate(evaluation_tasks, 1):
    print(f"\nüìã √âVALUATION {i}: {task_data['name']}")
    print(f"üìù INPUT: {task_data['input']}")
    print(f"üéØ OUTPUT ATTENDU: {task_data['expected_output']}")
    print("-" * 60)
    
    # Mesurer le temps de l'√©valuation
    evaluation_start_time = time.time()
    
    # Cr√©er l'√©valuation AccuracyEval
    evaluation = AccuracyEval(
        name=task_data['name'],
        model=xAI(id="grok-3", api_key=XAI_API_KEY),  # Mod√®le pour l'√©valuation
        agent=agent,  # Agent √† √©valuer
        input=task_data['input'],
        expected_output=task_data['expected_output'],
        additional_guidelines=task_data['additional_guidelines'],
        num_iterations=3,  # Nombre d'it√©rations pour la robustesse
    )
    
    # Ex√©cuter l'√©valuation
    result: Optional[AccuracyResult] = evaluation.run(print_results=True)
    
    # Mesurer le temps de fin
    evaluation_end_time = time.time()
    total_evaluation_time = evaluation_end_time - evaluation_start_time
    
    # Collecter les m√©triques
    if result is not None:
        # R√©cup√©rer les scores individuels depuis les r√©sultats d√©taill√©s
        individual_scores = []
        if hasattr(result, 'results') and result.results:
            for eval_result in result.results:
                if hasattr(eval_result, 'score'):
                    individual_scores.append(eval_result.score)
        
        task_metrics = {
            'task_id': i,
            'task_name': task_data['name'],
            'input': task_data['input'],
            'expected_output': task_data['expected_output'],
            'avg_score': result.avg_score,
            'scores': individual_scores,
            'iterations': len(individual_scores) if individual_scores else 0,
            'execution_time': total_evaluation_time,
            'success': result.avg_score >= 7.0  # Seuil de succ√®s
        }
        
        accuracy_metrics.append(task_metrics)
        
           
        # Afficher les r√©sultats
        print(f"üìä Score moyen: {result.avg_score:.2f}/10")
        if individual_scores:
            print(f"   ‚Ä¢ Scores individuels: {individual_scores}")
            print(f"   ‚Ä¢ Score max: {max(individual_scores):.2f}")
            print(f"   ‚Ä¢ Score min: {min(individual_scores):.2f}")
        else:
            print(f"   ‚Ä¢ Scores individuels: Non disponibles")
        print(f"   ‚Ä¢ Succ√®s: {'‚úÖ' if result.avg_score >= 7.0 else '‚ùå'}")
        print(f"   ‚Ä¢ Temps d'ex√©cution: {total_evaluation_time:.3f}s")
        
    else:
        print("‚ùå √âchec de l'√©valuation - Aucun r√©sultat obtenu")
        task_metrics = {
            'task_id': i,
            'task_name': task_data['name'],
            'input': task_data['input'],
            'expected_output': task_data['expected_output'],
            'avg_score': 0.0,
            'scores': [],
            'iterations': 0,
            'execution_time': total_evaluation_time,
            'success': False
        }
        accuracy_metrics.append(task_metrics)

# Analyse globale de l'accuracy evaluation
print("\n" + "="*80)
print("üìä ANALYSE DE L'ACCURACY EVALUATION")
print("="*80)

if accuracy_metrics:
    total_avg_score = sum(m['avg_score'] for m in accuracy_metrics)
    average_accuracy = total_avg_score / len(accuracy_metrics)
    total_execution_time = sum(m['execution_time'] for m in accuracy_metrics)
    successful_tasks = sum(1 for m in accuracy_metrics if m['success'])
    success_rate = (successful_tasks / len(accuracy_metrics)) * 100

    print(f"\nüìä R√âSULTATS GLOBAUX:")
    print(f"   ‚Ä¢ Score moyen d'accuracy: {average_accuracy:.2f}/10")
    print(f"   ‚Ä¢ Score total: {total_avg_score:.2f}")
    print(f"   ‚Ä¢ Taux de succ√®s: {success_rate:.1f}% ({successful_tasks}/{len(accuracy_metrics)})")
    print(f"   ‚Ä¢ Temps total d'ex√©cution: {total_execution_time:.3f}s")
    print(f"   ‚Ä¢ Nombre d'√©valuations: {len(accuracy_metrics)}")

    # Analyse par composant
    all_scores = []
    for m in accuracy_metrics:
        all_scores.extend(m['scores'])
    
    if all_scores:
        max_score = max(all_scores)
        min_score = min(all_scores)
        score_std = (sum((s - average_accuracy)**2 for s in all_scores) / len(all_scores))**0.5
        
        print(f"\nüîç ANALYSE D√âTAILL√âE:")
        print(f"   ‚Ä¢ Score maximum: {max_score:.2f}/10")
        print(f"   ‚Ä¢ Score minimum: {min_score:.2f}/10")
        print(f"   ‚Ä¢ √âcart-type: {score_std:.2f}")
        print(f"   ‚Ä¢ Total d'it√©rations: {len(all_scores)}")

    # Cr√©er un r√©sum√© des r√©sultats
    results_summary = {
        "average_accuracy": average_accuracy,
        "total_avg_score": total_avg_score,
        "success_rate": success_rate,
        "total_execution_time": total_execution_time,
        "successful_tasks": successful_tasks,
        "total_tasks": len(accuracy_metrics),
        "max_score": max_score if all_scores else 0,
        "min_score": min_score if all_scores else 0,
        "score_std": score_std if all_scores else 0
    }

else:
    print("‚ùå Aucune m√©trique d'accuracy disponible")

print("="*80)
print("üìä √âvaluation termin√©e - Consultez les r√©sultats ci-dessus")
print("="*80)

